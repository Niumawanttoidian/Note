## 链式求导法

先介绍下链式求导法则，在后面的反向传播算法中会用到。

有![李宏毅机器学习——深度学习反向传播算法_深度学习](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20y%20%3D%20g%28x%29%2Cz%3Dh%28y%29)

那么![李宏毅机器学习——深度学习反向传播算法_深度学习_02](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7Bdz%7D%7Bdx%7D%20%3D%20%5Cfrac%7Bdz%7D%7Bdy%7D%5Cfrac%7Bdy%7D%7Bdx%7D);

有![李宏毅机器学习——深度学习反向传播算法_反向传播算法_03](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20x%3Dg%28s%29%20%2Cy%20%3D%20h%28s%29%2C%20z%20%3D%20k%28x%2Cy%29)

![李宏毅机器学习——深度学习反向传播算法_反向传播算法_04](https://s2.51cto.com/images/blog/202207/13165222_62ce87c6c54a223298.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

改变了s会改变x和y，从而改变了z。

![李宏毅机器学习——深度学习反向传播算法_神经网络_05](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7Bdz%7D%7Bds%7D%20%3D%20%5Cfrac%7B%5Cpartial%7Bz%7D%7D%7B%5Cpartial%7Bx%7D%7D%5Cfrac%7Bdx%7D%7Bds%7D%20%2B%20%5Cfrac%7B%5Cpartial%7Bz%7D%7D%7B%5Cpartial%7By%7D%7D%5Cfrac%7Bdy%7D%7Bds%7D)

注意，如果改变s会改变多个变量，它们的关系也是成立的。

## 损失函数

![李宏毅机器学习——深度学习反向传播算法_激活函数_06](https://s2.51cto.com/images/blog/202207/13165223_62ce87c72b81483608.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

假设给定一组参数![李宏毅机器学习——深度学习反向传播算法_神经网络_07](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Ctheta),把一个训练数据![李宏毅机器学习——深度学习反向传播算法_反向传播算法_08](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20x%5En)代入NN(神经网络)中，会得到输出![李宏毅机器学习——深度学习反向传播算法_神经网络_09](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20y%5En)。

![李宏毅机器学习——深度学习反向传播算法_反向传播算法_10](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20C%5En)是输出![李宏毅机器学习——深度学习反向传播算法_神经网络_09](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20y%5En)和实际![李宏毅机器学习——深度学习反向传播算法_激活函数_12](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Chat%7By%7D%5En)距离函数，值越大代表越距离远，也就是效果越不好。

那在神经网络训练算法中，损失函数定义为：

![李宏毅机器学习——深度学习反向传播算法_反向传播算法_13](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%20%20L%28%5Ctheta%29%20%3D%20%5Csum%5E%7BN%7D_%7Bn%3D1%7DC%5En%28%5Ctheta%29%20)

如果把损失函数对参数![李宏毅机器学习——深度学习反向传播算法_神经网络_14](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20w)做微分的话，得到

![李宏毅机器学习——深度学习反向传播算法_神经网络_15](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20L%28%5Ctheta%29%7D%7B%5Cpartial%20w%7D%20%3D%20%5Csum%5E%7BN%7D_%7Bn%3D1%7D%5Cfrac%7B%5Cpartial%20C%5En%28%5Ctheta%29%7D%7B%5Cpartial%20w%7D%20)

只要计算出某一笔数据对![李宏毅机器学习——深度学习反向传播算法_神经网络_14](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20w)的微分，就可以得到![李宏毅机器学习——深度学习反向传播算法_反向传播算法_17](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20L%28%5Ctheta%29)对![李宏毅机器学习——深度学习反向传播算法_神经网络_14](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20w)的微分。

![李宏毅机器学习——深度学习反向传播算法_深度学习_19](https://s2.51cto.com/images/blog/202207/13165223_62ce87c77059b49349.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

假设我们先考虑这个神经元。

![李宏毅机器学习——深度学习反向传播算法_激活函数_20](https://s2.51cto.com/images/blog/202207/13165224_62ce87c80235f5073.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

假设只有两个输入![李宏毅机器学习——深度学习反向传播算法_深度学习_21](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20x_1%2Cx_2)，计算![李宏毅机器学习——深度学习反向传播算法_激活函数_22](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20z%20%3D%20x_1w_1%20%2B%20x_2w_2%20%2B%20b)得到![李宏毅机器学习——深度学习反向传播算法_神经网络_23](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20z)后再代入激活函数，经过多次运算会得到最终的输出![李宏毅机器学习——深度学习反向传播算法_反向传播算法_24](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20y_1%2Cy_2)。

![李宏毅机器学习——深度学习反向传播算法_反向传播算法_25](https://s2.51cto.com/images/blog/202207/13165224_62ce87c844c7439487.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

现在问题是如何计算损失(距离函数)![李宏毅机器学习——深度学习反向传播算法_反向传播算法_26](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20C)对![李宏毅机器学习——深度学习反向传播算法_神经网络_14](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20w)的偏微分![李宏毅机器学习——深度学习反向传播算法_神经网络_28](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20w%7D)

利用链式求导法

![李宏毅机器学习——深度学习反向传播算法_深度学习_29](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20w%7D%20%3D%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%7D%20%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20w%7D%20)

计算![李宏毅机器学习——深度学习反向传播算法_神经网络_30](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20w%7D)的过程叫做**正向过程(Forward pass)**；计算![李宏毅机器学习——深度学习反向传播算法_反向传播算法_31](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%7D)的过程叫做**反向过程(Backward pass)**。

## 正向过程

![李宏毅机器学习——深度学习反向传播算法_激活函数_22](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20z%20%3D%20x_1w_1%20%2B%20x_2w_2%20%2B%20b)

![李宏毅机器学习——深度学习反向传播算法_反向传播算法_33](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20w_1%7D%20%3D%20x_1%20%5C%5C%20%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20w_2%7D%20%3D%20x_2%20%5C%5C%20)

![李宏毅机器学习——深度学习反向传播算法_深度学习_34](https://s2.51cto.com/images/blog/202207/13165224_62ce87c886fea13067.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

如上图所示，假设输入是![李宏毅机器学习——深度学习反向传播算法_深度学习_35](https://math-api.51cto.com/?from=%20%20%20%20%20%20%201%2C-1)，上面蓝色神经元的参数：![李宏毅机器学习——深度学习反向传播算法_神经网络_36](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20w_1%3D1%2Cw_2%3D-2%2Cb%3D1)，激活函数是`Sigmoid`函数；
下面蓝色神经元的参数：![李宏毅机器学习——深度学习反向传播算法_激活函数_37](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20w_1%3D-1%2Cw_2%3D1%2Cb%3D0)

对下面的神经元来说，计算![李宏毅机器学习——深度学习反向传播算法_深度学习_38](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20w_2)的偏微分，可以很快得出![李宏毅机器学习——深度学习反向传播算法_神经网络_39](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20w%7D%20%3D%20-1)，也就是输入![李宏毅机器学习——深度学习反向传播算法_激活函数_40](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20x_2%28-1%29),随着从前往后计算每个神经元的输出，整个过程就可以很快结束，因此叫正向过程。

## 反向过程

![李宏毅机器学习——深度学习反向传播算法_神经网络_41](https://s2.51cto.com/images/blog/202207/13165225_62ce87c905ccf45097.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

困难的是如何计算![李宏毅机器学习——深度学习反向传播算法_反向传播算法_31](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%7D)

> ![李宏毅机器学习——深度学习反向传播算法_神经网络_43](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%20a%20%3D%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D)

假设激活函数是`Sigmoid`函数![李宏毅机器学习——深度学习反向传播算法_神经网络_44](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20a%3D%5Csigma%28z%29)，然后得到的函数值![李宏毅机器学习——深度学习反向传播算法_激活函数_45](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20a)会乘上某个权重(比如![李宏毅机器学习——深度学习反向传播算法_反向传播算法_46](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20w_3))再加上其他值得到![李宏毅机器学习——深度学习反向传播算法_激活函数_47](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20z%5E%5Cprime)(注意这里只是一个符号，不是![李宏毅机器学习——深度学习反向传播算法_神经网络_23](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20z)的导数)；![李宏毅机器学习——深度学习反向传播算法_激活函数_45](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20a)也会乘上权重(比如![李宏毅机器学习——深度学习反向传播算法_深度学习_50](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20w_4))再加上其他东西得到![李宏毅机器学习——深度学习反向传播算法_深度学习_51](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20z%5E%7B%5Cprime%5Cprime%7D)(注意这里只是一个符号，不是![李宏毅机器学习——深度学习反向传播算法_神经网络_23](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20z)的二阶导数)；

![李宏毅机器学习——深度学习反向传播算法_反向传播算法_53](https://s2.51cto.com/images/blog/202207/13165225_62ce87c94df0135231.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

![李宏毅机器学习——深度学习反向传播算法_神经网络_54](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%7D%20%3D%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20a%7D%20%5Cfrac%7B%5Cpartial%20a%7D%7B%5Cpartial%20z%7D%20)

可以这样理解，![李宏毅机器学习——深度学习反向传播算法_神经网络_23](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20z)通过影响![李宏毅机器学习——深度学习反向传播算法_激活函数_45](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20a)来影响![李宏毅机器学习——深度学习反向传播算法_反向传播算法_26](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20C)。

而

![李宏毅机器学习——深度学习反向传播算法_激活函数_58](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20a%7D%7B%5Cpartial%20z%7D%20%3D%20%5Cfrac%7B%5Cpartial%20%5Csigma%28z%29%7D%7B%5Cpartial%20z%7D%20%3D%20%5Csigma%5E%5Cprime%28z%29%20)

那就剩下

![李宏毅机器学习——深度学习反向传播算法_反向传播算法_59](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20a%7D%20%3D%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%5Cprime%7D%5Cfrac%7B%5Cpartial%20z%5E%5Cprime%7D%7B%5Cpartial%20a%7D%20%2B%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7B%5Cprime%5Cprime%7D%7D%5Cfrac%7B%5Cpartial%20z%5E%7B%5Cprime%5Cprime%7D%7D%7B%5Cpartial%20a%7D%20)

改变了![李宏毅机器学习——深度学习反向传播算法_激活函数_45](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20a)会改变![李宏毅机器学习——深度学习反向传播算法_激活函数_61](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20z%5E%7B%5Cprime%7D)和![李宏毅机器学习——深度学习反向传播算法_深度学习_51](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20z%5E%7B%5Cprime%5Cprime%7D)，从而改变了![李宏毅机器学习——深度学习反向传播算法_反向传播算法_26](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20C)

我们先计算简单的

![李宏毅机器学习——深度学习反向传播算法_深度学习_64](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20z%5E%7B%5Cprime%7D%20%3D%20aw_3%20%2B%20%5Ccdots)

有
![李宏毅机器学习——深度学习反向传播算法_深度学习_65](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20z%5E%7B%5Cprime%7D%7D%7B%5Cpartial%20a%7D%20%3D%20w_3)

同理

![李宏毅机器学习——深度学习反向传播算法_神经网络_66](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20z%5E%7B%5Cprime%5Cprime%7D%7D%7B%5Cpartial%20a%7D%20%3D%20w_4)

现在难点就是![李宏毅机器学习——深度学习反向传播算法_反向传播算法_67](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%5Cprime%7D)和![李宏毅机器学习——深度学习反向传播算法_反向传播算法_68](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7B%5Cprime%5Cprime%7D%7D)

我们这里先假装我们知道这两项的值。然后整理下原来的式子:

![李宏毅机器学习——深度学习反向传播算法_激活函数_69](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%7D%20%3D%20%5Csigma%5E%5Cprime%28z%29%5Bw_3%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%5Cprime%7D%20%2B%20w_4%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7B%5Cprime%5Cprime%7D%7D%5D%20)

![李宏毅机器学习——深度学习反向传播算法_激活函数_70](https://s2.51cto.com/images/blog/202207/13165225_62ce87c9b84586567.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

假设有另外一个特殊的神经元，它是上图的样子，输入就是![李宏毅机器学习——深度学习反向传播算法_反向传播算法_67](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%5Cprime%7D)和![李宏毅机器学习——深度学习反向传播算法_反向传播算法_68](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7B%5Cprime%5Cprime%7D%7D)，它们分别乘以![李宏毅机器学习——深度学习反向传播算法_反向传播算法_46](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20w_3)和![李宏毅机器学习——深度学习反向传播算法_深度学习_50](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20w_4)，然后求和得到的结果再乘上![李宏毅机器学习——深度学习反向传播算法_反向传播算法_75](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Csigma%5E%5Cprime%28z%29)
就得到了![李宏毅机器学习——深度学习反向传播算法_反向传播算法_31](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%7D)

![李宏毅机器学习——深度学习反向传播算法_神经网络_23](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20z)在正向传播的过程中已经知道了，因此这里的![李宏毅机器学习——深度学习反向传播算法_反向传播算法_75](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Csigma%5E%5Cprime%28z%29)是一个常数。

说了这么多，还是没说怎么计算![李宏毅机器学习——深度学习反向传播算法_反向传播算法_67](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%5Cprime%7D)和![李宏毅机器学习——深度学习反向传播算法_反向传播算法_68](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7B%5Cprime%5Cprime%7D%7D)啊。别急，下面就开始计算。

这里要分两种情况考虑：

![李宏毅机器学习——深度学习反向传播算法_反向传播算法_81](https://s2.51cto.com/images/blog/202207/13165226_62ce87ca5a7d340697.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

情形一： 红色的两个神经元就是输出层，它们能直接得到输出。

根据链式法则有：

![李宏毅机器学习——深度学习反向传播算法_神经网络_82](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%5Cprime%7D%20%3D%20%5Cfrac%7B%5Cpartial%20y_1%7D%7B%5Cpartial%20z%5E%5Cprime%7D%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20y_1%7D%20)

只要知道激活函数是啥就能计算出![李宏毅机器学习——深度学习反向传播算法_反向传播算法_83](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20y_1%7D%7B%5Cpartial%20z%5E%5Cprime%7D)

![李宏毅机器学习——深度学习反向传播算法_反向传播算法_84](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20y_1%7D)也可以根据我们选取的损失函数简单的计算出来。

同理![李宏毅机器学习——深度学习反向传播算法_反向传播算法_68](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7B%5Cprime%5Cprime%7D%7D)的计算也一样

情形二：红色的不是输出层

![李宏毅机器学习——深度学习反向传播算法_深度学习_86](https://s2.51cto.com/images/blog/202207/13165226_62ce87caceb7d675.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

红色的是中间层，它们的激活函数的值会当成下一层的输入继续参数计算。

![李宏毅机器学习——深度学习反向传播算法_激活函数_87](https://s2.51cto.com/images/blog/202207/13165227_62ce87cba8e1293229.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

如果我们知道![李宏毅机器学习——深度学习反向传播算法_反向传播算法_88](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z_a%7D)和![李宏毅机器学习——深度学习反向传播算法_反向传播算法_89](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z_b%7D)

同理(回顾一下上面那个特殊的神经元)我们就可以计算![李宏毅机器学习——深度学习反向传播算法_激活函数_90](https://math-api.51cto.com/?from=%20%20%20%20%20%20%20%5Cfrac%7B%5Cpartial%20C%7D%7B%5Cpartial%20z%5E%7B%5Cprime%7D%7D)

![李宏毅机器学习——深度学习反向传播算法_神经网络_91](https://s2.51cto.com/images/blog/202207/13165228_62ce87cc1ad3187123.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

问题就会这样反复循环下去，我们不停的看下一层，直到遇到了输出层。然后就可以由输出层往前计算出整个NN的所有的参数。

那我们为何不换个角度考虑问题，我们直接先算输出层的偏微分，然后依次往前计算。

![李宏毅机器学习——深度学习反向传播算法_反向传播算法_92](https://s2.51cto.com/images/blog/202207/13165228_62ce87cc81b4d24769.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184)

这就是反向传播算法的思想。