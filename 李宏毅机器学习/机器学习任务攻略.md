# [机器学习](https://so.csdn.net/so/search?q=机器学习&spm=1001.2101.3001.7020)任务攻略

## 1. 出现误差的原因

### 1.1 [model](https://so.csdn.net/so/search?q=model&spm=1001.2101.3001.7020) bias

- The model is too simple.
- Solution:redesign your model to make it more flexible.

![请添加图片描述](https://img-blog.csdnimg.cn/3e7213bb007747c0ba5ce357066680a9.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA57yW56iL5bCP5bCP55m955m9,size_16,color_FFFFFF,t_70,g_se,x_16)

### 1.2 optimization issue

- 局部最优解
  ![在这里插入图片描述](https://img-blog.csdnimg.cn/52bae6bbd9584b909290ecc288a54351.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA57yW56iL5bCP5bCP55m955m9,size_20,color_FFFFFF,t_70,g_se,x_16)从Testing data上来看，56层的表现不如20层，不是过拟合。检查一下训练资料上的结果，20层和56层对比，56层弹性应该比20层的大，但是表现的却不好，不是模型偏差，而是56层的optimization没有处理好 。
- Gaining the insights from camparison.
- Start from shallower networks(or other models), which are easier to
  optimize.
- If deeper networks do not obtain smaller loss on training data, then
  there is optimization issue.
  ![在这里插入图片描述](https://img-blog.csdnimg.cn/efb1ab597fb048b2a62c1424e3eee03a.png)
  上面数据中，5层的loss比4层的大，是不科学的，因此是optimization的问题。
- Solution：more powerful optimization technology

### 1.3 overfitting

small loss on training data, large loss on testing data.
An extreme example
如果x存在于训练集，就输出他的标签，否则输出一个随机数。
![在这里插入图片描述](https://img-blog.csdnimg.cn/a98e4cb38e66442781bac56758088244.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA57yW56iL5bCP5bCP55m955m9,size_20,color_FFFFFF,t_70,g_se,x_16)
更加灵活的模型会导致过[拟合](https://so.csdn.net/so/search?q=拟合&spm=1001.2101.3001.7020)的出现。![在这里插入图片描述](https://img-blog.csdnimg.cn/9c9f81995af24188b7d639d4d1221b56.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA57yW56iL5bCP5bCP55m955m9,size_20,color_FFFFFF,t_70,g_se,x_16)

解决办法

- 增加训练资料
- data augmentation（数据加强）
  根据对数据的理解，增加资料。将图片资料翻转，截取其中一部分。注意不要上下翻转（不符合常理，可能会使机器感到奇怪。）
- 对模型增加限制，不要让他过于灵活。
  *less parameters, sharing parameters.减少参数,如果是深度学习，就减少神经元的个数。
  less features 例如，用前三天的资料预测明天，可以减少为用前两天的资料来预测明天。
  early stopping
  regularization
  dropout*

![在这里插入图片描述](https://img-blog.csdnimg.cn/b1859de3085f439eaecd402d8538124d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA57yW56iL5bCP5bCP55m955m9,size_20,color_FFFFFF,t_70,g_se,x_16)

### 1.4 mismatch

training and testing data have different distributions.
![在这里插入图片描述](https://img-blog.csdnimg.cn/d7a907f89ac94dcc92cd1261bf8083f2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA57yW56iL5bCP5bCP55m955m9,size_20,color_FFFFFF,t_70,g_se,x_16)

## 2.训练集和测试集的划分

- 随机
- N-fold Cross validation
  ![在这里插入图片描述](https://img-blog.csdnimg.cn/932fbac798de4eb2975c9128e420b882.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA57yW56iL5bCP5bCP55m955m9,size_20,color_FFFFFF,t_70,g_se,x_16)

训练集：学习样本数据集，通过匹配一些参数建立一个分类器，主要是用来训练模型的。
验证集：对学习出来的模型，调整超参数。
测试集：测试集训练 ,好的模型的准确率