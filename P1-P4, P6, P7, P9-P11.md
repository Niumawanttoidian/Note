

## 

#### pytorch学习：

##### 1.环境配置

下载了mimconda，安装了配置了pytorch环境，并且再pycharm中配置好环境

![image-20230701201926366](C:\Users\sanercat\AppData\Roaming\Typora\typora-user-images\image-20230701201926366.png)

#### 2.pytorch的内容学习

##### 1.Tensor

torch中的Tensor是一种数据结构，类似python中二点list，numpy，当作多维数组来使用即可

1. Tensor可以进行的操作：类似numpy的API
   1. 索引
   2. 切片
   3. join：`cat(tensors)`或`stack(tensors)`
   4. 加法：`add()`或`+`
   5. 乘法：对元素层面的乘法`mul()`或`*`，矩阵乘法`matmul()`或`@`
   6. resize
      1. `reshape()`或`view()`
      2. `squeeze()`去掉长度为1的维度
      3. `unsqueeze()`增加一个维度（长度为1）
      4. `transpose()`转置2个维度
2. `Tensor.numpy()`可以将Tensor转换为numpy数据。反向的操作见上面序号2部分。
   注意这两方向的转换的数据对象都是占用同一储存空间，修改后变化也会体现在另一对象上。
3. `item()`函数返回仅有一个元素的Tensor的该元素值。

   3.如何生成Tensor？
torch包中提供了一系列直接生成Tensor的函数，如 `zeros()`、`ones()`、`rand()` 等。
此外，可以用 `tensor(data)` 函数直接将**某一表示数组的数据**（接受`list`、`numpy.ndarray`等格式）转换为Tensor。
也可以通过 `from_numpy(data)` 函数将**numpy.ndarray**格式的数据转换为Tensor。还可以生成一个与**其他Tensor**具有相同dtype和device等属性的Tensor，使用torch的 `ones_like(data)` 或`rand_like(data)` 等函数，或Tensor的 `new_ones()` 等函数

数据的生成（ jupyter文档）

数据的基本运算操作（ jupyter文档）

数据的索引（ jupyter文档）

##### 2.自动求导

1. torch.autograd是PyTorch提供的自动求导包，非常好用，可以不用自己算神经网络偏导了。

2. 神经网络构成、常识部分这里就不再详细介绍了，总之大概就是：

   1. 神经网络由权重、偏置等参数决定的函数构成，这些参数在PyTorch中都储存在Tensor里
   2. 神经网络的训练包括**前向传播**和**反向传播**两部分，前向传播就是用函数计算预测值，反向传播就是通过这一预测值产生的error/loss来更新参数（通过梯度下降的方式）

   1. 前向传播：`prediction = model(data)`
   2. 反向传播
      1. 计算loss
      2. `loss.backward()`（autograd会在这一步计算参数的梯度，存在相应参数Tensor的grad属性中）
      3. 更新参数
         1. 加载optimizer（通过torch.optim）
         2. `optimizer.step()`对参数使用梯度下降的方法进行更新（梯度来源自参数的grad属性）

> 本节以下内容都属于原理部分，可以直接跳过

1. autograd实现细节：一个示例

   1. 将Tensor的requires_grad属性设置为True，可以追踪autograd在其上每一步的操作
   2. 示例中，提供了两个requires_grad为True的Tensor（含两个元素的向量）a和b，设其损失函数Q = 3 a 3 − b 2 Q = 3a^3 - b^2Q=3a3−b2
   3. 注意：对Q计算梯度时，需要在`backward()`函数中添加gradient参数，这个gradient是和当前Tensor形状相同的Tensor，包含当前Tensor的梯度，比如示例中使用的是：d Q d Q = 1 \frac{dQ}{dQ} = 1dQdQ=1（因为Q是向量而非标量，参考`backward()`的[文档](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.backward)。为了避免这个问题也可以直接将Q转化为标量然后使用`backward()`方法，如`Q.sum().backward()`）
   4. 计算梯度：
      `external_grad = torch.tensor([1., 1.])`
      `Q.backward(gradient=external_grad)`
   5. 现在Q相对于a和b的梯度向量就分别储存在了a.grad和b.grad中，可以直接查看

2. 将Tensor的requires_grad属性设置为False，可以将其排除在DAG之外，autograd就不会计算它的梯度。
   在神经网络中，这种不需要计算梯度的参数叫frozen parameters。可以冻结不需要知道梯度的参数（节省计算资源），也可以在微调预训练模型时使用（此时往往冻结绝大多数参数，仅调整classifier layer参数，以在新标签上做预测）。
   类似功能也可以用上下文管理器`torch.no_grad()`实现。

3. ##### 神经网络

   1. 神经网络可以通过torch.nn包搭建（torch.nn包里预定义的层调用了torch.nn.functional包的函数）
   2. nn.Module包含了网络层
   3. `forward(input)`方法返回输出结果
   4. 示例：简单前馈神经网络convnet





### 李宏毅机器学习：

P1-P4：

# part1.基本介绍

## 1.机器学习的三个任务

一般情况下，我们在机器学习中有三个基本任务，分别是Regression Classification和Structured

Regression是计算数值解

而Classification则是求离散解（分类），也就是做选择题

Structured则是找一个结构，这种结构除了数据结构，还包括文字、绘画等广义的结构或者说某种意义上，让机器学习了之后创造一种东西出来

## 2.找函数三步骤

### 2.1写出一个带有为止参数的函数

![image](https://user-images.githubusercontent.com/102945300/180217011-7a979c24-dea0-4e4c-9087-4ce668fb88a8.png)

也就是面对一些我们位置的问题，仙写出一个带有未知参数的函数，也就是先猜测一下我们要如何求这个问题的解，比如一下这个简单的线性函数：

![image](https://user-images.githubusercontent.com/102945300/180217224-67013c99-d3ea-4fb2-98d2-9a1405903848.png)

![image](https://user-images.githubusercontent.com/102945300/180217242-6877d077-3659-409f-9fe6-e1221f9d4830.png)

这个猜测来自于作者对问题本质的了解，我们管这种带未知参数的函数为模型，也就是数学模型

### 2.2 定义Loss

第二部要定义一个Loss

![image](https://user-images.githubusercontent.com/102945300/180217569-6e447273-9794-43ef-878d-4f2bc473e3fb.png)

这个所谓的Loss其实就是一个Function，输入的值就是我们定义的未知参数，在上述就是我们给定的b和w（参数，变量是x），这个Loss输出的值代表 现在如果我们把这一组未知的参数设定为某个值的时候，这个数值好还是差。

就像我们在计算一组数据的线性回归方程的时候，是不是会有一部分和实际上的值偏差？某种意义上来说，这个Loss函数就是在对这个偏差值的情况做出评价

![image](https://user-images.githubusercontent.com/102945300/180218400-d29b8d82-69a9-411b-8453-41626471a233.png)

这里我们可以把每个离散值的误差加起来然后求个平均（原文中引用的视频播放量作为参考，但我觉得这个比较好理解，就随便放张图了）

![image](https://user-images.githubusercontent.com/102945300/180218653-828ed751-c9b7-405f-82b1-cc79ecde0fa2.png)

这种就是绝对差值叫MAE Mean Absolute error 还有就是把这个插值的平方算出来的，就叫Mean Square error，或者MSE，作业利用的MAR比较多

![image](https://user-images.githubusercontent.com/102945300/180220676-4accca20-e80d-418f-baef-b6c85e2026cc.png)

计算出来得到一个Error surface

## 2.3 优化

当然了，第三步就是优化问题，最佳化问题的结果，其实也就是我们要去找最好的w和b，让loss最小化

在这们课程里我们会用到的优化方式就只有梯度下降，现在我们简化一下，假设我们的未知参数只有一个，就是w

那当我们有不同的w，就会有不同的 loss，这时候我们的error surface就是一条曲线，如下图：

![image](https://user-images.githubusercontent.com/102945300/180221241-b7da5034-f9bf-4c0b-b517-66956a791d72.png)

那我们要怎么去找这么一个w呢？

![image](https://user-images.githubusercontent.com/102945300/180221297-7331157a-b59c-4861-b976-56456b3119a0.png)

看着复杂，其实也就是跟着这条曲线的斜率变化去做调整：斜率为负，则向右找；斜率为正，就向左找

那每次走多少呢？这就涉及到一个调参了：

![image](https://user-images.githubusercontent.com/102945300/180221899-c4fee0e1-30a2-415b-9ae6-48ff6af7dbb4.png)

注：loss是自己定义的，所以可以是负的

当然这里老师也注意到了局部最优解的问题，其实我也想到了，就是如果只根据这个逼近“极小值”的方法，其实是找不到最小值的，这个确实是梯度下降方法的一个问题。下面是老师的一段那话：

![image](https://user-images.githubusercontent.com/102945300/180222795-c13392af-a704-42c4-b9a6-f565d71e1166.png)

现在我们回到之前的有两个参数的模型，也就是有w和b的那个，这时候我们如何梯度下降呢？

![image](https://user-images.githubusercontent.com/102945300/180223440-92f21fbc-9cfa-4c2a-80fb-bf0dc6501ef2.png)

这个η就是我们定义的步长，或者说叫learning rate

![image](https://user-images.githubusercontent.com/102945300/180224632-c56ff1c0-f1e6-486d-8466-3c48dee5771a.png)

其实由上可以看出，整个loss的收敛方向其实是朝着 多个维度进行的，并不是单指一个方向，当多个维度下的数据都有不同的方向时，其效应就像带有引力的洞一样，会将我们的点向最深不见底的洞吸引过去。

什么时候停下来？两种情况

1.一开始就设定好最多迭代多少次，设定好迭代次数

2.最好的情况就是直接找到了极小值 w' = 0，当然这个...

### 线性模型

以上三个步骤：定义模型，写出loss，优化参数这三个步骤合起来就被我们称作训练，当然了，这个训练是在我们知道答案的基础上进行的，但是这并不是我们想要的，某种意义上来说这只是对过去规律的总结，真正对于我们重要的是未来的发展，预知未来。

那我们来拿着数据来预测一下试试看

![image](https://user-images.githubusercontent.com/102945300/180225961-53c89896-827b-46ca-afd0-39ceca300832.png)

然后我们发现，真实的数据和我们预测的数据 还是有很大的差距的，实际的数据有一定的周期，周末看的人多，工作日看的人少，然后七天一个循环

那我们假设给定它一个这样的参数列表：

![image](https://user-images.githubusercontent.com/102945300/180226757-16941c0e-c5d4-440c-abc2-fcc5da843262.png)

我们之前只考虑了一天，那我们最低的loss是0.58k的误差，其实这里计算出来的误差是0.33k，这两个数据至今的差距，就不言而喻了。

那我们考虑一个月，甚至考虑一年，又怎么样呢？我们管这种模型称作线性模型，之后会浅谈怎么把线性模型做得更好。

# 2.引入深度学习

在机器学习的第一步中通过linear models画出的线永远是一条直线，无法画出一条折线/曲线（红线）。Linear models:y=b+∑wj*xj
![示例：pandas 是基于NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。](https://img-blog.csdnimg.cn/3013472bda9b4527af3ee909e5c9920d.png)
这种来自model的限制叫做Model Bias.
而当x和y的关系很复杂时，为解决linear models,要写一个含有未知参数的函数Piecewise Linear。
![在这里插入图片描述](https://img-blog.csdnimg.cn/e4acff92eb1f4e6d8609804457217346.png)
可以把红线看做由一个常量+多个函数组成。
![在这里插入图片描述](https://img-blog.csdnimg.cn/6bc105e36e1c4aa1b29f66ce28c757cf.png)
可以用piecewise linear逼近任何有弧度的曲线，只要点取得够多/合适。每一段piecewise linear都可以用足够多的linear组合而成。
如何画出蓝色曲线：把蓝色线 看做[sigmoid](https://so.csdn.net/so/search?q=sigmoid&spm=1001.2101.3001.7020) function不断逼近的一条线。
函数内容：
![在这里插入图片描述](https://img-blog.csdnimg.cn/ba7eb33284274ff2bd7301f8a7692051.png)![在这里插入图片描述](https://img-blog.csdnimg.cn/ab99a4de39f64321a74d0364de5a7cdd.png)
不断调整b、w、c就可以调整出各种样子的sigmoid function。改变w：改变斜率/坡度。改变b：左右移动。改变c：改变高度。
![在这里插入图片描述](https://img-blog.csdnimg.cn/29041cf262e04dc88c87dd8079bb372c.png)
可以通过
![在这里插入图片描述](https://img-blog.csdnimg.cn/e2bf3b8298014224aa71192dcc94e54b.png)

来写一个linear model。
![在这里插入图片描述](https://img-blog.csdnimg.cn/04ada3b000804172b88837c100857ecd.png)
i表示第i个sigmoid function。
也可以看做
![在这里插入图片描述](https://img-blog.csdnimg.cn/8091ac8f94ae48828f70542e59f8c048.png)
向量r=向量b+向量w*向量x。
![在这里插入图片描述](https://img-blog.csdnimg.cn/5dbc9980dd1045b69e4ec2323213bc79.png)
而未知的所有参数（W，数值b，向量b，c[转置](https://so.csdn.net/so/search?q=转置&spm=1001.2101.3001.7020)）统称为Θ。

LOSS

类似上一篇 ，同样都是计算y和label（真正的）y的差值。L=1/N ∑ 差值。

找到使L最小的Θ

Θ=[Θ1，Θ2，Θ3…]T，随机选取一个Θ0
![在这里插入图片描述](https://img-blog.csdnimg.cn/8e41032946fc4c3694ab434e57bcc864.png)把L对Θi在Θ=Θ0时的微分集合起来叫做gradient，记作g=▽L（Θ0）。把Θ0更新为Θ1、Θ2、Θ3…直到不想做或结束为止。
![在这里插入图片描述](https://img-blog.csdnimg.cn/bab0efea85c34e02b6a5b743a3992b48.png)
实际上，不是一次性把L对Θ中所有变量梯度下降，而是把L随机分为多个batch（部分），每个部分分别计算▽Li（Θi）。对一部分梯度下降叫做update，完成一次对所有batch的叫做1epoch。

也可以不用soft sigmoid代替hard sigmoid。把他看做c*max(0,b+wx1)。这种线叫做Rectified Linear Unit(ReLU),把两个ReLU叠加就成为hard sigmoid。
![在这里插入图片描述](https://img-blog.csdnimg.cn/fefafcb8fa7848f4afc0eb3d8a48356c.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/83ee6bad40c848838f6e30acf56c8a61.png)
实验结果是ReLU更好，当ReLU个数越多Loss越小。
![在这里插入图片描述](https://img-blog.csdnimg.cn/fc61aca00f0d4be3a5942f0b1903f52c.png)
通过多次，效果更准。
![在这里插入图片描述](https://img-blog.csdnimg.cn/f847d82ff4e646a2847312d19f7f848b.png)
这些segmoid/ReLU叫做Neuron（神经元)，这些神经元叫做Neural Network。多个hidden layer叫做deep learning。

